{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecc3b48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "\n",
    "from ium_long_stay_patterns.src.helpers.create_numerical_dataset import create_numerical_dataset, merge_with_stats\n",
    "from ium_long_stay_patterns.config import ProcessedCSV, SAVED_MODELS_DIR\n",
    "from ium_long_stay_patterns.src.helpers.data_loaders import prepare_and_create_loaders\n",
    "from ium_long_stay_patterns.modeling.train import Trainer\n",
    "from models.binary import BinaryClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a54b20c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94f8dc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (1368, 18)\n",
      "Target distribution:\n",
      "target\n",
      "0    994\n",
      "1    374\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_numeric = create_numerical_dataset(ProcessedCSV.LISTINGS.path, strategy=True)\n",
    "df_final = merge_with_stats(df_numeric, with_ids=True)\n",
    "\n",
    "X = df_final.drop(columns=['target', 'id', 'host_id', 'listing_id'])\n",
    "y = df_final['target']\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Target distribution:\\n{y.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcfb8c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna to optimize.\n",
    "    Returns validation AUC score.\n",
    "    \"\"\"\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 3)\n",
    "\n",
    "    hidden_layers = []\n",
    "    for i in range(n_layers):\n",
    "        hidden_size = trial.suggest_int(f'n_units_l{i}', 16, 128, step=16)\n",
    "        hidden_layers.append(hidden_size)\n",
    "\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5, step=0.1)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)\n",
    "\n",
    "    train_loader, val_loader, _, _ = prepare_and_create_loaders(\n",
    "        X, y, batch_size=batch_size, random_state=42, save_test_data=False, verbose=False\n",
    "    )\n",
    "\n",
    "    data_iter = iter(train_loader)\n",
    "    sample_batch, _ = next(data_iter)\n",
    "    input_dim = sample_batch.shape[1]\n",
    "\n",
    "    model = BinaryClassifier(\n",
    "        input_dim=input_dim,\n",
    "        hidden_layers=hidden_layers,\n",
    "        dropout_rate=dropout_rate\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        epochs=10,\n",
    "        device=device,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    for epoch in range(1, 51):\n",
    "        trainer.model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            metrics = trainer._validate(val_loader)\n",
    "\n",
    "            trial.report(metrics['auc'], epoch)\n",
    "\n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    final_metrics = trainer._validate(val_loader)\n",
    "\n",
    "    return final_metrics['auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2fe684c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-13 20:00:52,952] A new study created in memory with name: binary_classifier_tuning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hyperparameter optimization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-13 20:01:05,702] Trial 0 finished with value: 0.8866876310272537 and parameters: {'n_layers': 3, 'n_units_l0': 112, 'n_units_l1': 32, 'n_units_l2': 80, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.0008970843137745766, 'batch_size': 32, 'weight_decay': 0.00041102629948232256}. Best is trial 0 with value: 0.8866876310272537.\n",
      "[I 2026-01-13 20:01:07,855] Trial 1 finished with value: 0.8580712788259959 and parameters: {'n_layers': 3, 'n_units_l0': 96, 'n_units_l1': 64, 'n_units_l2': 80, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.0009895388972497754, 'batch_size': 128, 'weight_decay': 0.0004134944748836937}. Best is trial 0 with value: 0.8866876310272537.\n",
      "[I 2026-01-13 20:01:10,865] Trial 2 finished with value: 0.7851153039832285 and parameters: {'n_layers': 1, 'n_units_l0': 112, 'dropout_rate': 0.4, 'learning_rate': 0.00021961140316981948, 'batch_size': 64, 'weight_decay': 1.5177644140769158e-05}. Best is trial 0 with value: 0.8866876310272537.\n",
      "[I 2026-01-13 20:01:14,549] Trial 3 finished with value: 0.9075471698113208 and parameters: {'n_layers': 3, 'n_units_l0': 128, 'n_units_l1': 96, 'n_units_l2': 128, 'dropout_rate': 0.0, 'learning_rate': 0.008613553761993107, 'batch_size': 64, 'weight_decay': 6.65076539985935e-06}. Best is trial 3 with value: 0.9075471698113208.\n",
      "[I 2026-01-13 20:01:21,110] Trial 4 finished with value: 0.9135220125786163 and parameters: {'n_layers': 2, 'n_units_l0': 64, 'n_units_l1': 112, 'dropout_rate': 0.1, 'learning_rate': 0.0015942117085017411, 'batch_size': 32, 'weight_decay': 5.6163464954033415e-06}. Best is trial 4 with value: 0.9135220125786163.\n",
      "[I 2026-01-13 20:01:21,474] Trial 5 pruned. \n",
      "[I 2026-01-13 20:01:22,516] Trial 6 pruned. \n",
      "[I 2026-01-13 20:01:23,647] Trial 7 pruned. \n",
      "[I 2026-01-13 20:01:24,920] Trial 8 pruned. \n",
      "[I 2026-01-13 20:01:25,685] Trial 9 pruned. \n",
      "[I 2026-01-13 20:01:31,300] Trial 10 finished with value: 0.9330188679245283 and parameters: {'n_layers': 2, 'n_units_l0': 16, 'n_units_l1': 128, 'dropout_rate': 0.0, 'learning_rate': 0.004910250673397578, 'batch_size': 32, 'weight_decay': 1.032514723545298e-06}. Best is trial 10 with value: 0.9330188679245283.\n",
      "[I 2026-01-13 20:01:36,341] Trial 11 finished with value: 0.8975890985324947 and parameters: {'n_layers': 2, 'n_units_l0': 16, 'n_units_l1': 128, 'dropout_rate': 0.0, 'learning_rate': 0.004481479838157412, 'batch_size': 32, 'weight_decay': 1.2769601235826328e-06}. Best is trial 10 with value: 0.9330188679245283.\n",
      "[I 2026-01-13 20:01:41,708] Trial 12 finished with value: 0.9232704402515723 and parameters: {'n_layers': 2, 'n_units_l0': 32, 'n_units_l1': 128, 'dropout_rate': 0.1, 'learning_rate': 0.0028617509130295575, 'batch_size': 32, 'weight_decay': 1.0026756671742826e-06}. Best is trial 10 with value: 0.9330188679245283.\n",
      "[I 2026-01-13 20:01:42,799] Trial 13 pruned. \n",
      "[I 2026-01-13 20:01:47,844] Trial 14 finished with value: 0.9048218029350105 and parameters: {'n_layers': 2, 'n_units_l0': 32, 'n_units_l1': 96, 'dropout_rate': 0.0, 'learning_rate': 0.003398947193848321, 'batch_size': 32, 'weight_decay': 2.8686486039567806e-06}. Best is trial 10 with value: 0.9330188679245283.\n",
      "[I 2026-01-13 20:01:48,217] Trial 15 pruned. \n",
      "[I 2026-01-13 20:01:50,790] Trial 16 pruned. \n",
      "[I 2026-01-13 20:01:55,796] Trial 17 finished with value: 0.9377358490566039 and parameters: {'n_layers': 2, 'n_units_l0': 16, 'n_units_l1': 112, 'dropout_rate': 0.0, 'learning_rate': 0.005315184658015584, 'batch_size': 32, 'weight_decay': 3.694197024296665e-05}. Best is trial 17 with value: 0.9377358490566039.\n",
      "[I 2026-01-13 20:01:56,112] Trial 18 pruned. \n",
      "[I 2026-01-13 20:01:56,963] Trial 19 pruned. \n",
      "[I 2026-01-13 20:02:02,345] Trial 20 finished with value: 0.9266247379454927 and parameters: {'n_layers': 2, 'n_units_l0': 48, 'n_units_l1': 112, 'dropout_rate': 0.0, 'learning_rate': 0.00605468483711744, 'batch_size': 32, 'weight_decay': 2.790473152498409e-05}. Best is trial 17 with value: 0.9377358490566039.\n",
      "[I 2026-01-13 20:02:07,794] Trial 21 finished with value: 0.9356394129979035 and parameters: {'n_layers': 2, 'n_units_l0': 48, 'n_units_l1': 112, 'dropout_rate': 0.0, 'learning_rate': 0.005944349994597196, 'batch_size': 32, 'weight_decay': 0.0008161083735621829}. Best is trial 17 with value: 0.9377358490566039.\n",
      "[I 2026-01-13 20:02:13,008] Trial 22 finished with value: 0.9171907756813418 and parameters: {'n_layers': 2, 'n_units_l0': 16, 'n_units_l1': 112, 'dropout_rate': 0.0, 'learning_rate': 0.008774301677133932, 'batch_size': 32, 'weight_decay': 0.0008328544894242362}. Best is trial 17 with value: 0.9377358490566039.\n",
      "[I 2026-01-13 20:02:14,165] Trial 23 pruned. \n",
      "[I 2026-01-13 20:02:16,728] Trial 24 pruned. \n",
      "[I 2026-01-13 20:02:20,211] Trial 25 pruned. \n",
      "[I 2026-01-13 20:02:21,136] Trial 26 pruned. \n",
      "[I 2026-01-13 20:02:22,406] Trial 27 pruned. \n",
      "[I 2026-01-13 20:02:25,375] Trial 28 finished with value: 0.9284067085953878 and parameters: {'n_layers': 2, 'n_units_l0': 80, 'n_units_l1': 112, 'dropout_rate': 0.2, 'learning_rate': 0.009644504156009528, 'batch_size': 64, 'weight_decay': 9.323005294978629e-05}. Best is trial 17 with value: 0.9377358490566039.\n",
      "[I 2026-01-13 20:02:25,786] Trial 29 pruned. \n",
      "[I 2026-01-13 20:02:26,800] Trial 30 pruned. \n",
      "[I 2026-01-13 20:02:29,832] Trial 31 finished with value: 0.9240041928721174 and parameters: {'n_layers': 2, 'n_units_l0': 80, 'n_units_l1': 112, 'dropout_rate': 0.2, 'learning_rate': 0.008963033685085175, 'batch_size': 64, 'weight_decay': 0.0001264171949194985}. Best is trial 17 with value: 0.9377358490566039.\n",
      "[I 2026-01-13 20:02:30,465] Trial 32 pruned. \n",
      "[I 2026-01-13 20:02:31,139] Trial 33 pruned. \n",
      "[I 2026-01-13 20:02:31,880] Trial 34 pruned. \n",
      "[I 2026-01-13 20:02:35,712] Trial 35 finished with value: 0.9205974842767295 and parameters: {'n_layers': 3, 'n_units_l0': 96, 'n_units_l1': 112, 'n_units_l2': 48, 'dropout_rate': 0.2, 'learning_rate': 0.009717611433892574, 'batch_size': 64, 'weight_decay': 4.7149731726696655e-05}. Best is trial 17 with value: 0.9377358490566039.\n",
      "[I 2026-01-13 20:02:36,293] Trial 36 pruned. \n",
      "[I 2026-01-13 20:02:36,676] Trial 37 pruned. \n",
      "[I 2026-01-13 20:02:37,839] Trial 38 pruned. \n",
      "[I 2026-01-13 20:02:38,931] Trial 39 pruned. \n",
      "[I 2026-01-13 20:02:39,675] Trial 40 pruned. \n",
      "[I 2026-01-13 20:02:40,764] Trial 41 pruned. \n",
      "[I 2026-01-13 20:02:46,219] Trial 42 finished with value: 0.920020964360587 and parameters: {'n_layers': 2, 'n_units_l0': 32, 'n_units_l1': 96, 'dropout_rate': 0.0, 'learning_rate': 0.005932417599151849, 'batch_size': 32, 'weight_decay': 3.8973615890360045e-06}. Best is trial 17 with value: 0.9377358490566039.\n",
      "[I 2026-01-13 20:02:51,707] Trial 43 pruned. \n",
      "[I 2026-01-13 20:02:52,921] Trial 44 pruned. \n",
      "[I 2026-01-13 20:02:54,013] Trial 45 pruned. \n",
      "[I 2026-01-13 20:02:55,103] Trial 46 pruned. \n",
      "[I 2026-01-13 20:02:57,310] Trial 47 pruned. \n",
      "[I 2026-01-13 20:02:57,710] Trial 48 pruned. \n",
      "[I 2026-01-13 20:02:58,938] Trial 49 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimization complete!\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10),\n",
    "    study_name='binary_classifier_tuning'\n",
    ")\n",
    "\n",
    "print(\"Starting hyperparameter optimization...\")\n",
    "study.optimize(objective, n_trials=50, timeout=3600)\n",
    "\n",
    "print(\"\\nOptimization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bf60e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Value (AUC): 0.9377\n",
      "\n",
      "  Params: \n",
      "    n_layers: 2\n",
      "    n_units_l0: 16\n",
      "    n_units_l1: 112\n",
      "    dropout_rate: 0.0\n",
      "    learning_rate: 0.005315184658015584\n",
      "    batch_size: 32\n",
      "    weight_decay: 3.694197024296665e-05\n"
     ]
    }
   ],
   "source": [
    "# Best trial\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(f\"  Value (AUC): {trial.value:.4f}\")\n",
    "print(\"\\n  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f26ae85",
   "metadata": {},
   "source": [
    "## Top 10 best trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a68118a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>value</th>\n",
       "      <th>params_batch_size</th>\n",
       "      <th>params_dropout_rate</th>\n",
       "      <th>params_learning_rate</th>\n",
       "      <th>params_n_layers</th>\n",
       "      <th>params_n_units_l0</th>\n",
       "      <th>params_n_units_l1</th>\n",
       "      <th>params_n_units_l2</th>\n",
       "      <th>params_weight_decay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.937736</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005315</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>112.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.935639</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005944</td>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>112.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.933019</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004910</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>128.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0.928407</td>\n",
       "      <td>64</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.009645</td>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>112.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.926625</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006055</td>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>112.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>0.924004</td>\n",
       "      <td>64</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.008963</td>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>112.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.923270</td>\n",
       "      <td>32</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.002862</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>128.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>0.920597</td>\n",
       "      <td>64</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.009718</td>\n",
       "      <td>3</td>\n",
       "      <td>96</td>\n",
       "      <td>112.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.000047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>0.920021</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005932</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>96.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.917191</td>\n",
       "      <td>32</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008774</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>112.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    number     value  params_batch_size  params_dropout_rate  \\\n",
       "17      17  0.937736                 32                  0.0   \n",
       "21      21  0.935639                 32                  0.0   \n",
       "10      10  0.933019                 32                  0.0   \n",
       "28      28  0.928407                 64                  0.2   \n",
       "20      20  0.926625                 32                  0.0   \n",
       "31      31  0.924004                 64                  0.2   \n",
       "12      12  0.923270                 32                  0.1   \n",
       "35      35  0.920597                 64                  0.2   \n",
       "42      42  0.920021                 32                  0.0   \n",
       "22      22  0.917191                 32                  0.0   \n",
       "\n",
       "    params_learning_rate  params_n_layers  params_n_units_l0  \\\n",
       "17              0.005315                2                 16   \n",
       "21              0.005944                2                 48   \n",
       "10              0.004910                2                 16   \n",
       "28              0.009645                2                 80   \n",
       "20              0.006055                2                 48   \n",
       "31              0.008963                2                 80   \n",
       "12              0.002862                2                 32   \n",
       "35              0.009718                3                 96   \n",
       "42              0.005932                2                 32   \n",
       "22              0.008774                2                 16   \n",
       "\n",
       "    params_n_units_l1  params_n_units_l2  params_weight_decay  \n",
       "17              112.0                NaN             0.000037  \n",
       "21              112.0                NaN             0.000816  \n",
       "10              128.0                NaN             0.000001  \n",
       "28              112.0                NaN             0.000093  \n",
       "20              112.0                NaN             0.000028  \n",
       "31              112.0                NaN             0.000126  \n",
       "12              128.0                NaN             0.000001  \n",
       "35              112.0               48.0             0.000047  \n",
       "42               96.0                NaN             0.000004  \n",
       "22              112.0                NaN             0.000833  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = study.trials_dataframe()\n",
    "\n",
    "df_sorted = df.sort_values(\"value\", ascending=False)\n",
    "\n",
    "top10 = df_sorted.head(10)\n",
    "\n",
    "cols = [\"number\", \"value\"] + [c for c in df_sorted.columns if c.startswith(\"params_\")]\n",
    "\n",
    "display(top10[cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0aed6b2",
   "metadata": {},
   "source": [
    "Params saved in *config.py*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ec5859",
   "metadata": {},
   "source": [
    "# Train best model - in *classification_numeric_data.ipynb*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ium-long-stay-patterns-kVcCAgnW-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
