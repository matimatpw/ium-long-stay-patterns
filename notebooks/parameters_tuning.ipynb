{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc3b48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "\n",
    "from ium_long_stay_patterns.src.helpers.create_numerical_dataset import create_numerical_dataset, merge_with_stats\n",
    "from ium_long_stay_patterns.config import ProcessedCSV, SAVED_MODELS_DIR\n",
    "from ium_long_stay_patterns.src.helpers.data_loaders import prepare_and_create_loaders\n",
    "from ium_long_stay_patterns.modeling.train import Trainer\n",
    "from models.binary import BinaryClassifier\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a54b20c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94f8dc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (1368, 21)\n",
      "Target distribution:\n",
      "target\n",
      "0    994\n",
      "1    374\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_numeric = create_numerical_dataset(ProcessedCSV.LISTINGS.path, strategy=True)\n",
    "df_final = merge_with_stats(df_numeric, with_ids=True)\n",
    "\n",
    "X = df_final.drop(columns=['target'])\n",
    "y = df_final['target']\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Target distribution:\\n{y.value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcfb8c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna to optimize.\n",
    "    Returns validation AUC score.\n",
    "    \"\"\"\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 3)\n",
    "\n",
    "    hidden_layers = []\n",
    "    for i in range(n_layers):\n",
    "        hidden_size = trial.suggest_int(f'n_units_l{i}', 16, 128, step=16)\n",
    "        hidden_layers.append(hidden_size)\n",
    "\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5, step=0.1)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)\n",
    "\n",
    "    train_loader, val_loader, _, _ = prepare_and_create_loaders(\n",
    "        X, y, batch_size=batch_size, random_state=42, save_test_data=False, verbose=False\n",
    "    )\n",
    "\n",
    "    data_iter = iter(train_loader)\n",
    "    sample_batch, _ = next(data_iter)\n",
    "    input_dim = sample_batch.shape[1]\n",
    "\n",
    "    model = BinaryClassifier(\n",
    "        input_dim=input_dim,\n",
    "        hidden_layers=hidden_layers,\n",
    "        dropout_rate=dropout_rate\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        epochs=10,\n",
    "        device=device,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    for epoch in range(1, 51):\n",
    "        trainer.model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            metrics = trainer._validate(val_loader)\n",
    "\n",
    "            trial.report(metrics['auc'], epoch)\n",
    "\n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    final_metrics = trainer._validate(val_loader)\n",
    "\n",
    "    return final_metrics['auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2fe684c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-13 19:06:09,658] A new study created in memory with name: binary_classifier_tuning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hyperparameter optimization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-13 19:06:19,021] Trial 0 finished with value: 0.8742662473794549 and parameters: {'n_layers': 3, 'n_units_l0': 80, 'n_units_l1': 80, 'n_units_l2': 96, 'dropout_rate': 0.1, 'learning_rate': 0.005874330820558931, 'batch_size': 32, 'weight_decay': 0.0007493818318346564}. Best is trial 0 with value: 0.8742662473794549.\n",
      "[I 2026-01-13 19:06:20,671] Trial 1 finished with value: 0.8024109014675053 and parameters: {'n_layers': 2, 'n_units_l0': 16, 'n_units_l1': 48, 'dropout_rate': 0.0, 'learning_rate': 0.0006756124400855356, 'batch_size': 128, 'weight_decay': 2.437568942663016e-05}. Best is trial 0 with value: 0.8742662473794549.\n",
      "[I 2026-01-13 19:06:22,458] Trial 2 finished with value: 0.880293501048218 and parameters: {'n_layers': 2, 'n_units_l0': 48, 'n_units_l1': 128, 'dropout_rate': 0.1, 'learning_rate': 0.0036490117975616736, 'batch_size': 128, 'weight_decay': 9.595923096526728e-05}. Best is trial 2 with value: 0.880293501048218.\n",
      "[I 2026-01-13 19:06:28,918] Trial 3 finished with value: 0.9018867924528302 and parameters: {'n_layers': 3, 'n_units_l0': 48, 'n_units_l1': 16, 'n_units_l2': 32, 'dropout_rate': 0.5, 'learning_rate': 0.006990547736828359, 'batch_size': 32, 'weight_decay': 1.113351799970423e-06}. Best is trial 3 with value: 0.9018867924528302.\n",
      "[I 2026-01-13 19:06:35,058] Trial 4 finished with value: 0.8687631027253668 and parameters: {'n_layers': 3, 'n_units_l0': 32, 'n_units_l1': 128, 'n_units_l2': 64, 'dropout_rate': 0.1, 'learning_rate': 0.004451847285789845, 'batch_size': 32, 'weight_decay': 2.5492877920666617e-05}. Best is trial 3 with value: 0.9018867924528302.\n",
      "[I 2026-01-13 19:06:35,965] Trial 5 pruned. \n",
      "[I 2026-01-13 19:06:36,970] Trial 6 pruned. \n",
      "[I 2026-01-13 19:06:37,571] Trial 7 pruned. \n",
      "[I 2026-01-13 19:06:38,154] Trial 8 pruned. \n",
      "[I 2026-01-13 19:06:38,466] Trial 9 pruned. \n",
      "[I 2026-01-13 19:06:39,722] Trial 10 pruned. \n",
      "[I 2026-01-13 19:06:40,145] Trial 11 pruned. \n",
      "[I 2026-01-13 19:06:40,466] Trial 12 pruned. \n",
      "[I 2026-01-13 19:06:40,833] Trial 13 pruned. \n",
      "[I 2026-01-13 19:06:41,518] Trial 14 pruned. \n",
      "[I 2026-01-13 19:06:42,588] Trial 15 pruned. \n",
      "[I 2026-01-13 19:06:42,966] Trial 16 pruned. \n",
      "[I 2026-01-13 19:06:43,434] Trial 17 pruned. \n",
      "[I 2026-01-13 19:06:44,387] Trial 18 pruned. \n",
      "[I 2026-01-13 19:06:45,075] Trial 19 pruned. \n",
      "[I 2026-01-13 19:06:45,536] Trial 20 pruned. \n",
      "[I 2026-01-13 19:06:46,897] Trial 21 pruned. \n",
      "[I 2026-01-13 19:06:48,604] Trial 22 pruned. \n",
      "[I 2026-01-13 19:06:54,815] Trial 23 finished with value: 0.8787211740041929 and parameters: {'n_layers': 3, 'n_units_l0': 96, 'n_units_l1': 96, 'n_units_l2': 96, 'dropout_rate': 0.2, 'learning_rate': 0.004203354707886344, 'batch_size': 32, 'weight_decay': 0.00032744825213874113}. Best is trial 3 with value: 0.9018867924528302.\n",
      "[I 2026-01-13 19:06:56,115] Trial 24 pruned. \n",
      "[I 2026-01-13 19:06:57,373] Trial 25 pruned. \n",
      "[I 2026-01-13 19:07:04,784] Trial 26 finished with value: 0.8981132075471698 and parameters: {'n_layers': 3, 'n_units_l0': 64, 'n_units_l1': 128, 'n_units_l2': 80, 'dropout_rate': 0.2, 'learning_rate': 0.004599578882328828, 'batch_size': 32, 'weight_decay': 0.00031022103023347936}. Best is trial 3 with value: 0.9018867924528302.\n",
      "[I 2026-01-13 19:07:05,922] Trial 27 pruned. \n",
      "[I 2026-01-13 19:07:06,632] Trial 28 pruned. \n",
      "[I 2026-01-13 19:07:07,004] Trial 29 pruned. \n",
      "[I 2026-01-13 19:07:08,289] Trial 30 pruned. \n",
      "[I 2026-01-13 19:07:12,923] Trial 31 pruned. \n",
      "[I 2026-01-13 19:07:20,326] Trial 32 finished with value: 0.9013626834381552 and parameters: {'n_layers': 3, 'n_units_l0': 48, 'n_units_l1': 96, 'n_units_l2': 96, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.007650701536174604, 'batch_size': 32, 'weight_decay': 0.00023845319615328924}. Best is trial 3 with value: 0.9018867924528302.\n",
      "[I 2026-01-13 19:07:27,863] Trial 33 finished with value: 0.9150943396226415 and parameters: {'n_layers': 3, 'n_units_l0': 16, 'n_units_l1': 128, 'n_units_l2': 112, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.007725376837724869, 'batch_size': 32, 'weight_decay': 0.0002422914901161402}. Best is trial 33 with value: 0.9150943396226415.\n",
      "[I 2026-01-13 19:07:35,922] Trial 34 finished with value: 0.9251572327044025 and parameters: {'n_layers': 3, 'n_units_l0': 16, 'n_units_l1': 80, 'n_units_l2': 112, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.008379799996070235, 'batch_size': 32, 'weight_decay': 0.000589230988413491}. Best is trial 34 with value: 0.9251572327044025.\n",
      "[I 2026-01-13 19:07:44,199] Trial 35 finished with value: 0.9311320754716982 and parameters: {'n_layers': 3, 'n_units_l0': 16, 'n_units_l1': 80, 'n_units_l2': 128, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.008395842005153897, 'batch_size': 32, 'weight_decay': 0.0005661499251740309}. Best is trial 35 with value: 0.9311320754716982.\n",
      "[I 2026-01-13 19:07:47,374] Trial 36 pruned. \n",
      "[I 2026-01-13 19:07:51,840] Trial 37 pruned. \n",
      "[I 2026-01-13 19:07:53,433] Trial 38 pruned. \n",
      "[I 2026-01-13 19:07:54,946] Trial 39 pruned. \n",
      "[I 2026-01-13 19:07:56,495] Trial 40 pruned. \n",
      "[I 2026-01-13 19:08:00,429] Trial 41 pruned. \n",
      "[I 2026-01-13 19:08:06,857] Trial 42 finished with value: 0.9459119496855346 and parameters: {'n_layers': 3, 'n_units_l0': 16, 'n_units_l1': 80, 'n_units_l2': 128, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.007446685760734886, 'batch_size': 32, 'weight_decay': 0.0005810024533824051}. Best is trial 42 with value: 0.9459119496855346.\n",
      "[I 2026-01-13 19:08:13,232] Trial 43 finished with value: 0.9313417190775681 and parameters: {'n_layers': 3, 'n_units_l0': 16, 'n_units_l1': 64, 'n_units_l2': 128, 'dropout_rate': 0.30000000000000004, 'learning_rate': 0.0061526504831153515, 'batch_size': 32, 'weight_decay': 0.0004041328417553098}. Best is trial 42 with value: 0.9459119496855346.\n",
      "[I 2026-01-13 19:08:14,491] Trial 44 pruned. \n",
      "[I 2026-01-13 19:08:17,038] Trial 45 pruned. \n",
      "[I 2026-01-13 19:08:18,369] Trial 46 pruned. \n",
      "[I 2026-01-13 19:08:19,050] Trial 47 pruned. \n",
      "[I 2026-01-13 19:08:20,339] Trial 48 pruned. \n",
      "[I 2026-01-13 19:08:22,899] Trial 49 pruned. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimization complete!\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10),\n",
    "    study_name='binary_classifier_tuning'\n",
    ")\n",
    "\n",
    "print(\"Starting hyperparameter optimization...\")\n",
    "study.optimize(objective, n_trials=50, timeout=3600)\n",
    "\n",
    "print(\"\\nOptimization complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bf60e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Value (AUC): 0.9459\n",
      "\n",
      "  Params: \n",
      "    n_layers: 3\n",
      "    n_units_l0: 16\n",
      "    n_units_l1: 80\n",
      "    n_units_l2: 128\n",
      "    dropout_rate: 0.30000000000000004\n",
      "    learning_rate: 0.007446685760734886\n",
      "    batch_size: 32\n",
      "    weight_decay: 0.0005810024533824051\n"
     ]
    }
   ],
   "source": [
    "# Best trial\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(f\"  Value (AUC): {trial.value:.4f}\")\n",
    "print(\"\\n  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ec5859",
   "metadata": {},
   "source": [
    "# Train best model - in *classification_numeric_data.ipynb*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e38ddaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best hyperparameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Reconstruct hidden layers\n",
    "n_layers = best_params['n_layers']\n",
    "hidden_layers = [best_params[f'n_units_l{i}'] for i in range(n_layers)]\n",
    "\n",
    "print(f\"Training final model with architecture: {hidden_layers}\")\n",
    "\n",
    "# Create data loaders with best batch size\n",
    "train_loader, val_loader, test_loader, fitted_scaler = prepare_and_create_loaders(\n",
    "    X, y, batch_size=best_params['batch_size'], random_state=42\n",
    ")\n",
    "\n",
    "# Get input dimension\n",
    "data_iter = iter(train_loader)\n",
    "sample_batch, _ = next(data_iter)\n",
    "input_dim = sample_batch.shape[1]\n",
    "\n",
    "# Create final model\n",
    "final_model = BinaryClassifier(\n",
    "    input_dim=input_dim,\n",
    "    hidden_layers=hidden_layers,\n",
    "    dropout_rate=best_params['dropout_rate']\n",
    ").to(device)\n",
    "\n",
    "# Create optimizer and criterion\n",
    "final_optimizer = optim.Adam(\n",
    "    final_model.parameters(),\n",
    "    lr=best_params['learning_rate'],\n",
    "    weight_decay=best_params['weight_decay']\n",
    ")\n",
    "final_criterion = nn.BCELoss()\n",
    "\n",
    "# Create trainer\n",
    "final_trainer = Trainer(\n",
    "    model=final_model,\n",
    "    criterion=final_criterion,\n",
    "    optimizer=final_optimizer,\n",
    "    epochs=100,  # Full training\n",
    "    device=device,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Train\n",
    "final_trainer.train(train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4d618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_trainer.save_model(SAVED_MODELS_DIR / \"best_binary_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ium-long-stay-patterns-kVcCAgnW-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
